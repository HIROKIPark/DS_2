{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### year,month,day,birth year, birth month를 같은 범주형 데이터로 넣어서 처리하여 확인함. \n",
    "- 단순 범주형 포함해서 라벨 인코딩 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 학습에 사용되는 자잘한 것들\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    모델 구조 수정 금지.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_dim, cat_features, num_features, num_classes, cat_cardinalities):\n",
    "        super(BaseModel, self).__init__()\n",
    "        # cat_cardinalities는 각 범주형 변수의 고유값 개수 리스트\n",
    "        self.cat_embeddings = nn.ModuleList([nn.Embedding(cardinality, 5) for cardinality in cat_cardinalities])\n",
    "        self.fc_cat = nn.Linear(len(cat_features) * 5 + len(num_features), 64)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        # Apply embedding layers\n",
    "        embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "        #print('len(embeddings : )',len(embeddings))\n",
    "        #print('len(x_num) : ',len(x_num))\n",
    "        x = torch.cat(embeddings + [x_num], dim=1)\n",
    "        #print('len(x) : ',len(x))\n",
    "        x = self.fc_cat(x)\n",
    "        encoded = self.encoder(x)\n",
    "        out = self.classifier(encoded)\n",
    "        # print(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_standardize_data(data, mode):\n",
    "    label_encoders = {}\n",
    "    cat_cardinalities = []\n",
    "\n",
    "    # 기존 범주형 열 정의\n",
    "    categorical_columns_train = ['Card Brand', 'Card Type', 'Card Number', 'Expires', 'Acct Open Date', 'Is Fraud?', 'Error Message']\n",
    "    categorical_columns_test = ['Card Brand', 'Card Type', 'Card Number', 'Expires', 'Acct Open Date', 'Error Message']\n",
    "    data['Error Message'] = data['Error Message'].fillna('None')\n",
    "    categorical_columns = categorical_columns_train if mode == 'Train' else categorical_columns_test\n",
    "\n",
    "    # 추가 범주형 열 정의 (Year, Month 등)\n",
    "    additional_categorical_columns = ['Year', 'Month', 'Day', 'Birth Year', 'Birth Month']\n",
    "    categorical_columns += additional_categorical_columns  # 기존 열과 합침\n",
    "\n",
    "    # 모든 범주형 열에 대해 레이블 인코딩 수행\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "        cat_cardinalities.append(data[col].nunique())\n",
    "\n",
    "    # Zipcode와 Merchandise Code 처리 (레이블 인코딩)\n",
    "    for col in ['Zipcode', 'Merchandise Code']:\n",
    "        data[col] = (data[col] // 100).astype(int)\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "        cat_cardinalities.append(data[col].nunique())\n",
    "\n",
    "    # Boolean 열 처리 (Has Chip)\n",
    "    data['Has Chip'] = np.where(data['Has Chip'] == True, 1, 0)\n",
    "    cat_cardinalities.append(data['Has Chip'].nunique())\n",
    "\n",
    "    # 연속형 열만 정규화\n",
    "    continuous_columns = [\n",
    "        'Current Age', 'Retirement Age', 'Per Capita Income - Zipcode',\n",
    "        'Yearly Income', 'Total Debt', 'Credit Score', 'Credit Limit', 'Amount'\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    data[continuous_columns] = scaler.fit_transform(data[continuous_columns])\n",
    "\n",
    "    # 범주형 및 수치형 열 분리\n",
    "    categorical_columns += ['Zipcode', 'Merchandise Code', 'Has Chip']\n",
    "    cat_features = data[categorical_columns].astype(int)  # 범주형은 정수형\n",
    "    num_features = data[continuous_columns]  # 연속형 데이터\n",
    "\n",
    "    return cat_features, num_features, cat_cardinalities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "x_cat_train, x_num_train, cat_cardinalities_train = encode_and_standardize_data(train_data, mode='Train')\n",
    "x_cat_test, x_num_test, cat_cardinalities_test = encode_and_standardize_data(test_data, mode='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card Brand             2\n",
      "Card Type              1\n",
      "Card Number         3088\n",
      "Expires               57\n",
      "Acct Open Date       203\n",
      "Error Message         20\n",
      "Year                   0\n",
      "Month                  0\n",
      "Day                    5\n",
      "Birth Year            43\n",
      "Birth Month           10\n",
      "Zipcode               52\n",
      "Merchandise Code      24\n",
      "Has Chip               1\n",
      "Name: 1, dtype: int64\n",
      "Current Age                    0.055478\n",
      "Retirement Age                -0.105149\n",
      "Per Capita Income - Zipcode    0.458919\n",
      "Yearly Income                  1.550286\n",
      "Total Debt                     1.607348\n",
      "Credit Score                   0.694038\n",
      "Credit Limit                   0.744991\n",
      "Amount                         0.189164\n",
      "Name: 1, dtype: float64\n",
      "[3, 3, 3850, 60, 297, 22, 1, 12, 31, 78, 12, 555, 37, 2]\n"
     ]
    }
   ],
   "source": [
    "print(x_cat_test.iloc[1], x_num_test.iloc[1], cat_cardinalities_test,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensor로 변환\n",
    "x_cat_train_tensor = torch.tensor(x_cat_train.values, dtype=torch.long)  # 정수형\n",
    "x_num_train_tensor = torch.tensor(x_num_train.values, dtype=torch.float32)  # 실수형\n",
    "\n",
    "x_cat_test_tensor = torch.tensor(x_cat_test.values, dtype=torch.long)\n",
    "x_num_test_tensor = torch.tensor(x_num_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0330, 0.1393],\n",
      "        [0.0364, 0.1385],\n",
      "        [0.0382, 0.1343],\n",
      "        ...,\n",
      "        [0.0525, 0.1234],\n",
      "        [0.0458, 0.1283],\n",
      "        [0.0504, 0.1237]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2  # 예: Is Fraud? 이진 분류\n",
    "encoding_dim = 64 # 이게 이제 \n",
    "\n",
    "model = BaseModel(encoding_dim=encoding_dim, cat_features=x_cat_train.columns, num_features=x_num_train.columns, num_classes=num_classes, cat_cardinalities=cat_cardinalities_train)\n",
    "\n",
    "# 모델 출력 테스트\n",
    "output = model(x_cat_train_tensor, x_num_train_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-Test Split\n",
    "x_cat_train_split, x_cat_val_split, x_num_train_split, x_num_val_split = train_test_split(\n",
    "    x_cat_train_tensor, x_num_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset 및 DataLoader 정의\n",
    "train_dataset = torch.utils.data.TensorDataset(x_cat_train_split, x_num_train_split)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_cat_val_split, x_num_val_split)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Loss Function, Optimizer 정의\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류의 경우\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train Loop 구현\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x_cat_batch, x_num_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            outputs = model(x_cat_batch, x_num_batch)\n",
    "            loss = criterion(outputs, torch.randint(0, 2, (x_cat_batch.size(0),)))  # 임시 타겟 (예: binary class)\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_cat_batch, x_num_batch in val_loader:\n",
    "                outputs = model(x_cat_batch, x_num_batch)\n",
    "                loss = criterion(outputs, torch.randint(0, 2, (x_cat_batch.size(0),)))  # 임시 타겟\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        model.train()  # 다시 학습 모드로 전환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 14255.9634\n",
      "Validation Loss: 3563.7038\n",
      "Epoch 2/10, Loss: 14255.6973\n",
      "Validation Loss: 3563.6864\n",
      "Epoch 3/10, Loss: 14256.4961\n",
      "Validation Loss: 3564.0176\n",
      "Epoch 4/10, Loss: 14255.7034\n",
      "Validation Loss: 3563.9158\n",
      "Epoch 5/10, Loss: 14255.9462\n",
      "Validation Loss: 3564.1151\n",
      "Epoch 6/10, Loss: 14256.5603\n",
      "Validation Loss: 3563.8032\n",
      "Epoch 7/10, Loss: 14255.9508\n",
      "Validation Loss: 3563.9766\n",
      "Epoch 8/10, Loss: 14255.5785\n",
      "Validation Loss: 3563.8093\n",
      "Epoch 9/10, Loss: 14255.7446\n",
      "Validation Loss: 3564.4433\n",
      "Epoch 10/10, Loss: 14255.8782\n",
      "Validation Loss: 3564.2597\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x_cat_test_tensor, x_num_test_tensor)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Outputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x_cat, x_num)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_cat, x_num):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply embedding layers\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [emb(x_cat[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_embeddings)]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print('len(embeddings : )',len(embeddings))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print('len(x_num) : ',len(x_num))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings \u001b[38;5;241m+\u001b[39m [x_num], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_cat, x_num):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply embedding layers\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [emb(x_cat[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_embeddings)]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print('len(embeddings : )',len(embeddings))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print('len(x_num) : ',len(x_num))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings \u001b[38;5;241m+\u001b[39m [x_num], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "model = BaseModel(\n",
    "    encoding_dim=64,\n",
    "    cat_features=x_cat_train.columns,\n",
    "    num_features=x_num_train.columns,\n",
    "    num_classes=2,  # 이진 분류\n",
    "    cat_cardinalities=cat_cardinalities_train\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "# Test Data 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_cat_test_tensor, x_num_test_tensor)\n",
    "    print(\"Test Outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
