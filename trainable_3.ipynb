{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실패 한 것에 대한 정리)\n",
    "- 정규화 처리 과정 변경 -> 기존에 시도하려 했지만 차원 수도 늘어나고 복잡해지면서 문제가 생기는 듯함.\n",
    "- 주기적 데이터(Month, Day): sin, cos 변환으로 원형 관계를 반영.\n",
    "- 범주형 데이터\n",
    "- 고유값이 적은 변수 → 원-핫 인코딩.\n",
    "- 고유값이 많은 변수 → 라벨 인코딩 또는 Embedding.\n",
    "이렇게 해보려고 하였으나 차원 수가 커지고 복잡해지는 문제가 생겨서 날짜 데이터를 수치적으로 가져가기 위해서 0~1로 정규화 시켜서 적용하였음. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화 처리 과정 변경된 것\n",
    "-  date_columns = ['Year', 'Month', 'Day', 'Birth Year', 'Birth Month']-> 날짜 데이터도 0~1로 표현되게 정규화 처리함.\n",
    "- 연속형 데이터: Min-Max Scaling으로 0~1로 정규화. -> ReLU는 음수 값을 0으로 바꿔버리기 때문에, StandardScaler에서 생성된 음수 값이 문제를 일으킬 가능성이 있을 수 있어서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 학습에 사용되는 자잘한 것들\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    모델 구조 수정 금지.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_dim, cat_features, num_features, num_classes, cat_cardinalities):\n",
    "        super(BaseModel, self).__init__()\n",
    "        # cat_cardinalities는 각 범주형 변수의 고유값 개수 리스트\n",
    "        self.cat_embeddings = nn.ModuleList([nn.Embedding(cardinality, 5) for cardinality in cat_cardinalities])\n",
    "        self.fc_cat = nn.Linear(len(cat_features) * 5 + len(num_features), 64)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        # Apply embedding layers\n",
    "        embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "        #print('len(embeddings : )',len(embeddings))\n",
    "        #print('len(x_num) : ',len(x_num))\n",
    "        x = torch.cat(embeddings + [x_num], dim=1)\n",
    "        #print('len(x) : ',len(x))\n",
    "        x = self.fc_cat(x)\n",
    "        encoded = self.encoder(x)\n",
    "        out = self.classifier(encoded)\n",
    "        # print(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_standardize_data(data, mode):\n",
    "    label_encoders = {}\n",
    "    cat_cardinalities = []\n",
    "\n",
    "    # 기존 범주형 열 정의\n",
    "    categorical_columns_train = ['Card Brand', 'Card Type', 'Card Number', 'Expires', 'Acct Open Date', 'Is Fraud?', 'Error Message']\n",
    "    categorical_columns_test = ['Card Brand', 'Card Type', 'Card Number', 'Expires', 'Acct Open Date', 'Error Message']\n",
    "    data['Error Message'] = data['Error Message'].fillna('None')\n",
    "    categorical_columns = categorical_columns_train if mode == 'Train' else categorical_columns_test\n",
    "\n",
    "    # 범주형 데이터 레이블 인코딩\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "        cat_cardinalities.append(data[col].nunique())\n",
    "\n",
    "    # Zipcode와 Merchandise Code 처리 (레이블 인코딩)\n",
    "    for col in ['Zipcode', 'Merchandise Code']:\n",
    "        data[col] = (data[col] // 100).astype(int)\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "        cat_cardinalities.append(data[col].nunique())\n",
    "\n",
    "    # Boolean 열 처리 (Has Chip)\n",
    "    data['Has Chip'] = np.where(data['Has Chip'] == True, 1, 0)\n",
    "    cat_cardinalities.append(data['Has Chip'].nunique())\n",
    "    \n",
    "    # 날짜 데이터 정규화\n",
    "    date_columns = ['Year', 'Month', 'Day', 'Birth Year', 'Birth Month']\n",
    "    for col in date_columns:\n",
    "        if data[col].max() != data[col].min():  # 값이 서로 다를 때만 정규화 수행\n",
    "            data[col] = (data[col] - data[col].min()) / (data[col].max() - data[col].min())\n",
    "        else:  # 값이 동일하면 0으로 설정\n",
    "            data[col] = 0\n",
    "\n",
    "    # 연속형 데이터 정규화\n",
    "    continuous_columns = [\n",
    "        'Current Age', 'Retirement Age', 'Per Capita Income - Zipcode',\n",
    "        'Yearly Income', 'Total Debt', 'Credit Score', 'Credit Limit', 'Amount'\n",
    "    ]\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()  # MinMaxScaler로 변경\n",
    "    data[continuous_columns] = scaler.fit_transform(data[continuous_columns])\n",
    "\n",
    "    # 범주형 및 수치형 열 분리\n",
    "    categorical_columns += ['Zipcode', 'Merchandise Code', 'Has Chip']\n",
    "    cat_features = data[categorical_columns].astype(int)  # Ensure categorical features are integer\n",
    "    num_features = data[continuous_columns + date_columns]  # 날짜와 연속형 데이터를 결합\n",
    "\n",
    "    return cat_features, num_features, cat_cardinalities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m x_cat_train, x_num_train, cat_cardinalities_train \u001b[38;5;241m=\u001b[39m encode_and_standardize_data(train_data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m x_cat_test, x_num_test, cat_cardinalities_test \u001b[38;5;241m=\u001b[39m encode_and_standardize_data(test_data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 19\u001b[0m, in \u001b[0;36mencode_and_standardize_data\u001b[0;34m(data, mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_columns:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m label_encoders:  \u001b[38;5;66;03m# 이미 훈련 데이터에서 fit한 경우\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         data[col] \u001b[38;5;241m=\u001b[39m label_encoders[col]\u001b[38;5;241m.\u001b[39mtransform(data[col])\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 훈련 데이터의 경우\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 2"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "x_cat_train, x_num_train, cat_cardinalities_train = encode_and_standardize_data(train_data, mode='Train')\n",
    "x_cat_test, x_num_test, cat_cardinalities_test = encode_and_standardize_data(test_data, mode='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Card Brand             2\n",
      "Card Type              1\n",
      "Card Number         3088\n",
      "Expires               57\n",
      "Acct Open Date       203\n",
      "Error Message         20\n",
      "Zipcode               52\n",
      "Merchandise Code      24\n",
      "Has Chip               1\n",
      "Name: 1, dtype: int64\n",
      "Current Age                    0.414634\n",
      "Retirement Age                 0.551724\n",
      "Per Capita Income - Zipcode    0.179460\n",
      "Yearly Income                  0.329990\n",
      "Total Debt                     0.286566\n",
      "Credit Score                   0.574924\n",
      "Credit Limit                   0.171828\n",
      "Amount                         0.100280\n",
      "Year                           0.000000\n",
      "Month                          0.000000\n",
      "Day                            0.166667\n",
      "Birth Year                     0.585366\n",
      "Birth Month                    0.909091\n",
      "Name: 1, dtype: float64\n",
      "[3, 3, 3850, 60, 297, 22, 555, 37, 2]\n"
     ]
    }
   ],
   "source": [
    "print(x_cat_test.iloc[1], x_num_test.iloc[1], cat_cardinalities_test,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensor로 변환\n",
    "x_cat_train_tensor = torch.tensor(x_cat_train.values, dtype=torch.long)  # 정수형\n",
    "x_num_train_tensor = torch.tensor(x_num_train.values, dtype=torch.float32)  # 실수형\n",
    "\n",
    "x_cat_test_tensor = torch.tensor(x_cat_test.values, dtype=torch.long)\n",
    "x_num_test_tensor = torch.tensor(x_num_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2268, -0.2595],\n",
      "        [ 0.2205, -0.2598],\n",
      "        [ 0.2246, -0.2628],\n",
      "        ...,\n",
      "        [ 0.2278, -0.2448],\n",
      "        [ 0.2338, -0.2504],\n",
      "        [ 0.2336, -0.2514]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2  # 예: Is Fraud? 이진 분류\n",
    "encoding_dim = 64 # 이게 이제 \n",
    "\n",
    "model = BaseModel(encoding_dim=encoding_dim, cat_features=x_cat_train.columns, num_features=x_num_train.columns, num_classes=num_classes, cat_cardinalities=cat_cardinalities_train)\n",
    "\n",
    "# 모델 출력 테스트\n",
    "output = model(x_cat_train_tensor, x_num_train_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-Test Split\n",
    "x_cat_train_split, x_cat_val_split, x_num_train_split, x_num_val_split = train_test_split(\n",
    "    x_cat_train_tensor, x_num_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset 및 DataLoader 정의\n",
    "train_dataset = torch.utils.data.TensorDataset(x_cat_train_split, x_num_train_split)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_cat_val_split, x_num_val_split)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Loss Function, Optimizer 정의\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류의 경우\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train Loop 구현\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x_cat_batch, x_num_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            outputs = model(x_cat_batch, x_num_batch)\n",
    "            loss = criterion(outputs, torch.randint(0, 2, (x_cat_batch.size(0),)))  # 임시 타겟 (예: binary class)\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_cat_batch, x_num_batch in val_loader:\n",
    "                outputs = model(x_cat_batch, x_num_batch)\n",
    "                loss = criterion(outputs, torch.randint(0, 2, (x_cat_batch.size(0),)))  # 임시 타겟\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        model.train()  # 다시 학습 모드로 전환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 14252.4794\n",
      "Validation Loss: 3563.1953\n",
      "Epoch 2/10, Loss: 14252.3776\n",
      "Validation Loss: 3563.2070\n",
      "Epoch 3/10, Loss: 14252.5173\n",
      "Validation Loss: 3563.0763\n",
      "Epoch 4/10, Loss: 14252.3241\n",
      "Validation Loss: 3563.2234\n",
      "Epoch 5/10, Loss: 14252.1432\n",
      "Validation Loss: 3563.1689\n",
      "Epoch 6/10, Loss: 14252.3973\n",
      "Validation Loss: 3563.0662\n",
      "Epoch 7/10, Loss: 14252.4731\n",
      "Validation Loss: 3563.2320\n",
      "Epoch 8/10, Loss: 14252.0650\n",
      "Validation Loss: 3562.9845\n",
      "Epoch 9/10, Loss: 14252.4512\n",
      "Validation Loss: 3563.1127\n",
      "Epoch 10/10, Loss: 14252.5042\n",
      "Validation Loss: 3563.0195\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x_cat_test_tensor, x_num_test_tensor)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Outputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# t-SNE 분석\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[39], line 31\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x_cat, x_num)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_cat, x_num):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply embedding layers\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [emb(x_cat[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_embeddings)]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print('len(embeddings : )',len(embeddings))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print('len(x_num) : ',len(x_num))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings \u001b[38;5;241m+\u001b[39m [x_num], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_cat, x_num):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Apply embedding layers\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [emb(x_cat[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_embeddings)]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#print('len(embeddings : )',len(embeddings))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print('len(x_num) : ',len(x_num))\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings \u001b[38;5;241m+\u001b[39m [x_num], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
