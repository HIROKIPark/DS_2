{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### year,month,day,birth year, birth month를 같은 범주형 데이터로 넣어서 처리하여 확인함. \n",
    "- 라벨 인코딩 말고 원-핫 인코딩된 데이터를 그대로 사용하기 위해서 self.cat_embeddings = nn.ModuleList([nn.Embedding(cardinality, 5) for cardinality in cat_cardinalities])\n",
    "- 근데 원-핫 인코딩이 너무 안됨.. 일단 계속 돌려보려고 했지만 그냥 너무 안됨.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 Import\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 학습에 사용되는 자잘한 것들\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    모델 구조 수정 금지.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoding_dim, cat_features, num_features, num_classes, cat_cardinalities):\n",
    "        super(BaseModel, self).__init__()\n",
    "        # cat_cardinalities는 각 범주형 변수의 고유값 개수 리스트\n",
    "        self.cat_embeddings = nn.ModuleList([nn.Embedding(cardinality, 5) for cardinality in cat_cardinalities])\n",
    "        self.fc_cat = nn.Linear(len(cat_features) * 5 + len(num_features), 64)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        # Apply embedding layers\n",
    "        embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "        #print('len(embeddings : )',len(embeddings))\n",
    "        #print('len(x_num) : ',len(x_num))\n",
    "        x = torch.cat(embeddings + [x_num], dim=1)\n",
    "        #print('len(x) : ',len(x))\n",
    "        x = self.fc_cat(x)\n",
    "        encoded = self.encoder(x)\n",
    "        out = self.classifier(encoded)\n",
    "        # print(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def encode_and_standardize_data(data, mode):\n",
    "    # OneHotEncoder 설정\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    cat_cardinalities = []\n",
    "\n",
    "    # 기존 범주형 열 정의\n",
    "    categorical_columns_train = ['Card Brand', 'Card Type', 'Card Number', 'Expires', 'Acct Open Date', 'Is Fraud?', 'Error Message']\n",
    "    categorical_columns_test = ['Card Brand', 'Card Type', 'Card Number', 'Expires', 'Acct Open Date', 'Error Message']\n",
    "    data['Error Message'] = data['Error Message'].fillna('None')\n",
    "    categorical_columns = categorical_columns_train if mode == 'Train' else categorical_columns_test\n",
    "\n",
    "    # 추가 범주형 열 정의 (Year, Month 등)\n",
    "    additional_categorical_columns = ['Year', 'Month', 'Day', 'Birth Year', 'Birth Month']\n",
    "    categorical_columns += additional_categorical_columns  # 기존 열과 합침\n",
    "\n",
    "    # 모든 범주형 열에 대해 원-핫 인코딩 수행\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(data[categorical_columns])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(categorical_columns))\n",
    "    cat_cardinalities = [one_hot_df[col].nunique() for col in one_hot_df.columns]\n",
    "\n",
    "    # Zipcode와 Merchandise Code 처리 (레이블 인코딩)\n",
    "    for col in ['Zipcode', 'Merchandise Code']:\n",
    "        data[col] = (data[col] // 100).astype(int)\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        cat_cardinalities.append(data[col].nunique())\n",
    "\n",
    "    # Boolean 열 처리 (Has Chip)\n",
    "    data['Has Chip'] = np.where(data['Has Chip'] == True, 1, 0)\n",
    "    cat_cardinalities.append(data['Has Chip'].nunique())\n",
    "\n",
    "    # 연속형 열만 정규화\n",
    "    continuous_columns = [\n",
    "        'Current Age', 'Retirement Age', 'Per Capita Income - Zipcode',\n",
    "        'Yearly Income', 'Total Debt', 'Credit Score', 'Credit Limit', 'Amount'\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    data[continuous_columns] = scaler.fit_transform(data[continuous_columns])\n",
    "\n",
    "    # 범주형 및 수치형 열 분리\n",
    "    categorical_columns += ['Zipcode', 'Merchandise Code', 'Has Chip']\n",
    "    cat_features = pd.concat([one_hot_df, data[['Zipcode', 'Merchandise Code', 'Has Chip']]], axis=1)\n",
    "    num_features = data[continuous_columns]  # 연속형 데이터\n",
    "\n",
    "    return cat_features, num_features, cat_cardinalities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gidaseul/opt/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m x_cat_train, x_num_train, cat_cardinalities_train \u001b[38;5;241m=\u001b[39m encode_and_standardize_data(train_data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m x_cat_test, x_num_test, cat_cardinalities_test \u001b[38;5;241m=\u001b[39m encode_and_standardize_data(test_data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mencode_and_standardize_data\u001b[0;34m(data, mode)\u001b[0m\n\u001b[1;32m     21\u001b[0m one_hot_encoded \u001b[38;5;241m=\u001b[39m one_hot_encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[categorical_columns])\n\u001b[1;32m     22\u001b[0m one_hot_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(one_hot_encoded, columns\u001b[38;5;241m=\u001b[39mone_hot_encoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_columns))\n\u001b[0;32m---> 23\u001b[0m cat_cardinalities \u001b[38;5;241m=\u001b[39m [one_hot_df[col]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m one_hot_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Zipcode와 Merchandise Code 처리 (레이블 인코딩)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZipcode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerchandise Code\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m one_hot_encoded \u001b[38;5;241m=\u001b[39m one_hot_encoder\u001b[38;5;241m.\u001b[39mfit_transform(data[categorical_columns])\n\u001b[1;32m     22\u001b[0m one_hot_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(one_hot_encoded, columns\u001b[38;5;241m=\u001b[39mone_hot_encoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(categorical_columns))\n\u001b[0;32m---> 23\u001b[0m cat_cardinalities \u001b[38;5;241m=\u001b[39m [one_hot_df[col]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m one_hot_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Zipcode와 Merchandise Code 처리 (레이블 인코딩)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZipcode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMerchandise Code\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:1063\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[0;34m(self, dropna)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     uniqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropna:\n\u001b[1;32m   1065\u001b[0m         uniqs \u001b[38;5;241m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:2291\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m algorithms\u001b[38;5;241m.\u001b[39munique1d(values)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_with_mask(values)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39munique(values)\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "x_cat_train, x_num_train, cat_cardinalities_train = encode_and_standardize_data(train_data, mode='Train')\n",
    "x_cat_test, x_num_test, cat_cardinalities_test = encode_and_standardize_data(test_data, mode='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_cat_test.iloc[1], x_num_test.iloc[1], cat_cardinalities_test,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch tensor로 변환\n",
    "x_cat_train_tensor = torch.tensor(x_cat_train.values, dtype=torch.long)  # 정수형\n",
    "x_num_train_tensor = torch.tensor(x_num_train.values, dtype=torch.float32)  # 실수형\n",
    "\n",
    "x_cat_test_tensor = torch.tensor(x_cat_test.values, dtype=torch.long)\n",
    "x_num_test_tensor = torch.tensor(x_num_test.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # 예: Is Fraud? 이진 분류\n",
    "encoding_dim = 64 # 이게 이제 \n",
    "\n",
    "model = BaseModel(encoding_dim=encoding_dim, cat_features=x_cat_train.columns, num_features=x_num_train.columns, num_classes=num_classes, cat_cardinalities=cat_cardinalities_train)\n",
    "\n",
    "# 모델 출력 테스트\n",
    "output = model(x_cat_train_tensor, x_num_train_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-Test Split\n",
    "x_cat_train_split, x_cat_val_split, x_num_train_split, x_num_val_split = train_test_split(\n",
    "    x_cat_train_tensor, x_num_train_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Dataset 및 DataLoader 정의\n",
    "train_dataset = torch.utils.data.TensorDataset(x_cat_train_split, x_num_train_split)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_cat_val_split, x_num_val_split)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Loss Function, Optimizer 정의\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류의 경우\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train Loop 구현\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for x_cat_batch, x_num_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            outputs = model(x_cat_batch, x_num_batch)\n",
    "            loss = criterion(outputs, torch.randint(0, 2, (x_cat_batch.size(0),)))  # 임시 타겟 (예: binary class)\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 가중치 업데이트\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Validation Loop\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_cat_batch, x_num_batch in val_loader:\n",
    "                outputs = model(x_cat_batch, x_num_batch)\n",
    "                loss = criterion(outputs, torch.randint(0, 2, (x_cat_batch.size(0),)))  # 임시 타겟\n",
    "                val_loss += loss.item()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        model.train()  # 다시 학습 모드로 전환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = BaseModel(\n",
    "    encoding_dim=64,\n",
    "    cat_features=x_cat_train.columns,\n",
    "    num_features=x_num_train.columns,\n",
    "    num_classes=2,  # 이진 분류\n",
    "    cat_cardinalities=cat_cardinalities_train\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "# Test Data 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_cat_test_tensor, x_num_test_tensor)\n",
    "    print(\"Test Outputs:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
